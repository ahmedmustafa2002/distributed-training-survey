# distributed-training-survey

| **Parameter/Scenario**               | **DistributedDataParallel (DDP)**                                                                                                           | **PipelineParallel + DDP (PDP)**                                                                                                           | **FullyShardedDataParallel (FSDP)**                                                                                  |
|---------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| **Optimal Use Case**                  | Models < 3B parameters, low Amortized Communication Overhead (ACO).                                                                        | Models < 10B parameters, high ACO.                                                                                                        | Models > 10B parameters, scales to 1T parameters.                                                                    |
| **Scaling Efficiency**                | Best throughput for intra-machine setups. Performs better with small models.                                                               | Outperforms DDP in multi-machine setups due to smaller and concurrent AllReduce operations.                                                | Suitable for large-scale models, but throughput drops significantly with increased parameter offloading overhead.     |
| **Memory Optimization Techniques**    | Limited compatibility with parameter offloading.                                                                                           | Compatible with activation checkpointing and micro-batching.                                                                               | Supports activation checkpointing, activation offloading, parameter offloading, and micro-batching.                  |
| **Batch Size Impact**                 | Larger batch sizes increase throughput due to reduced amortized per-sample communication overhead.                                          | Similar behavior as DDP; smaller micro-batches reduce peak memory usage but slightly reduce throughput.                                    | High compatibility with large batch sizes and memory optimization techniques.                                         |
| **Network Bandwidth Sensitivity**     | Highly sensitive to network bandwidth, especially in multi-machine setups.                                                                 | Performs efficiently in multi-machine setups due to better utilization of bandwidth.                                                       | Performs poorly on slower networks due to parameter offloading and finer wrapping granularity.                        |
| **Throughput (Small Models)**         | Highest throughput in single-machine setups.                                                                                               | Slightly lower than DDP but benefits from reduced communication overhead in multi-machine configurations.                                  | Moderate throughput, better suited for larger models.                                                                 |
| **Throughput (Large Models)**         | Unable to handle models > 3B parameters due to memory limitations.                                                                          | Handles models up to ~34B parameters with optimized pipelines.                                                                             | Handles models > 10B parameters effectively, only paradigm tested for 1T parameter models.                           |
| **Peak Memory Usage**                 | High for large models due to full replication of parameters and activations on all GPUs.                                                    | Lower than DDP due to pipeline partitioning and micro-batching.                                                                            | Lowest memory usage due to parameter sharding and offloading techniques.                                              |
| **Impact of Network Optimization**    | Minor improvement with faster networks but limited by single AllReduce operations.                                                         | Less affected by faster networks, as concurrent AllReduce operations are already efficient.                                                | Significant improvement with network optimizations like NCCL Fast Socket, especially for large models.                |
| **Recommendations for Batch Size**    | Best performance with larger batch sizes for small models.                                                                                 | Supports smaller micro-batches for memory savings but at a slight cost to throughput.                                                     | Large batch sizes with memory optimization techniques yield the best results.                                         |
| **Maximum Model Size (A100 GPUs)**    | Up to 2.8B parameters without optimization.                                                                                                | Up to 34B parameters with 4-device pipeline and memory optimization.                                                                       | Scales to 1T parameters but requires 400 Gbps+ networks for practical throughput.                                      |
| **Limitations**                       | Inefficient for large models, high memory consumption, and low scalability for multi-machine setups.                                        | Pipeline length is limited by the number of GPUs; OOM issues for very large models even with memory optimization.                          | Significant performance drop with slower networks and fine-grained parameter offloading.                              |
| **Best Practices (Summary)**          | Use for small to medium models with fast interconnects (e.g., NVLink).                                                                     | Use for medium models in multi-node setups where ACO is high.                                                                              | Use for large models, especially with advanced memory and network optimizations.                                      |

---
